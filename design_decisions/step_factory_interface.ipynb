{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Version 1.0.5\n"
                    ]
                }
            ],
            "source": [
                "%load_ext nb_mypy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# to make imports and folder paths work\n",
                "# todo: Instead create python package and install locally\n",
                "import os, sys\n",
                "os.chdir(\n",
                "    f'{os.environ[\"HOME\"]}/repos/sagemaker-pipelines-abstraction/src'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
                        "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
                    ]
                }
            ],
            "source": [
                "from abc import ABC, abstractmethod\n",
                "from typing import Generic, TypeVar, TypedDict, Any\n",
                "from pathlib import Path\n",
                "\n",
                "from pydantic_settings import BaseSettings\n",
                "import boto3\n",
                "from sagemaker.processing import Processor, FrameworkProcessor\n",
                "from sagemaker.base_predictor import Predictor\n",
                "from sagemaker.workflow.steps import ConfigurableRetryStep, ProcessingStep\n",
                "from sagemaker.workflow.pipeline_context import _JobStepArguments\n",
                "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
                "from sagemaker.sklearn.estimator import SKLearn\n",
                "\n",
                "from sm_pipelines_oo.shared_config_schema import SharedConfig\n",
                "from sm_pipelines_oo.steps.step_utils import PathFactory\n",
                "from sm_pipelines_oo.utils import load_pydantic_config_from_file\n",
                "from sm_pipelines_oo.pipeline_wrapper import AWSConnectorInterface"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Alternative 1: Make ProcessingStepFactoryInterface *generic* in ProcessorType \n",
                "**Problem: Different types of Processors require different run_args (and potentially different extra_kwargs), which is not possible using this architecture.**\n",
                "\n",
                "We could try to solve this by adding two more generic type variables, RunArgsType and ExtraKwargsType, but this would not only overly complicate things, but also we would have to rely on the caller to pass a matching set of these three types variables. To solve this, we could define a concrete interface in a single place for each matching set of type variables, but it's probably easier to simply use subclassing instead of generics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# *General* step factory interface\n",
                "# ==============================\n",
                "class StepFactoryInterface(ABC):\n",
                "    @abstractmethod\n",
                "    def create_step(self, shared_config) -> ConfigurableRetryStep:\n",
                "        ...\n",
                "\n",
                "\n",
                "# Factory interfaces for *specific* step types\n",
                "# =============================================\n",
                "ProcessorType = TypeVar(\"ProcessorType\", bound=Processor)\n",
                "\n",
                "class ProcessingStepFactoryInterface(StepFactoryInterface, Generic[ProcessorType]):\n",
                "    @abstractmethod\n",
                "    def create_step(self, shared_config) -> ProcessingStep:\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def processor(self) -> ProcessorType:\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_processor_run_args(self, shared_config: SharedConfig) -> dict:\n",
                "        # todo: improve return type\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_processor_extra_kwargs(self, shared_config: SharedConfig) -> dict:\n",
                "        # todo: improve return type\n",
                "        ...\n",
                "\n",
                "\n",
                "# Use\n",
                "# ====\n",
                "framework_processing_step = ProcessingStepFactoryInterface[FrameworkProcessor]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Alternative 2: Define interface for ProcessorType, and implement for each specific kind of processor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# *General* step factory interface\n",
                "# ==============================\n",
                "from sagemaker.workflow.steps import ProcessingStep\n",
                "\n",
                "\n",
                "class BaseStepFactoryInterface(ABC):\n",
                "    @abstractmethod\n",
                "    def create_step(self, shared_config) -> ConfigurableRetryStep:\n",
                "        ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Factory interfaces for *processing* step\n",
                "# =============================================\n",
                "\n",
                "class ProcessingStepFactoryInterface(BaseStepFactoryInterface):\n",
                "    @abstractmethod\n",
                "    def create_step(self, shared_config: SharedConfig) -> ProcessingStep:\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def processor(self) -> Processor:\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_processor_run_args(self) -> dict:\n",
                "        # todo: improve return type\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_processor_extra_kwargs(self, shared_config: SharedConfig) -> dict:\n",
                "        # todo: improve return type\n",
                "        ..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Static analysis catches type mismatches\n",
                "Note that type checking does not work for the Sagemaker SDK by default, but I enabled it with the fix described [here](../../../design_decisions/typing_sagemaker_sdk.ipynb), which I have included in the setup defined in the [project's Makefile](../../../Makefile)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<cell>8: \u001b[1m\u001b[31merror:\u001b[m Return type \u001b[m\u001b[1m\"None\"\u001b[m of \u001b[m\u001b[1m\"create_step\"\u001b[m incompatible with return type \u001b[m\u001b[1m\"ProcessingStep\"\u001b[m in supertype \u001b[m\u001b[1m\"ProcessingStepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[override]\u001b[m\n",
                        "<cell>8: \u001b[1m\u001b[31merror:\u001b[m Return type \u001b[m\u001b[1m\"None\"\u001b[m of \u001b[m\u001b[1m\"create_step\"\u001b[m incompatible with return type \u001b[m\u001b[1m\"ConfigurableRetryStep\"\u001b[m in supertype \u001b[m\u001b[1m\"BaseStepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[override]\u001b[m\n",
                        "<cell>14: \u001b[1m\u001b[31merror:\u001b[m Return type \u001b[m\u001b[1m\"int\"\u001b[m of \u001b[m\u001b[1m\"processor\"\u001b[m incompatible with return type \u001b[m\u001b[1m\"Processor\"\u001b[m in supertype \u001b[m\u001b[1m\"ProcessingStepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[override]\u001b[m\n",
                        "<cell>18: \u001b[1m\u001b[31merror:\u001b[m Signature of \u001b[m\u001b[1m\"get_processor_run_args\"\u001b[m incompatible with supertype \u001b[m\u001b[1m\"ProcessingStepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[override]\u001b[m\n",
                        "<cell>18: \u001b[34mnote:\u001b[m      Superclass:\u001b[m\n",
                        "<cell>18: \u001b[34mnote:\u001b[m          def get_processor_run_args(self) -> dict[Any, Any]\u001b[m\n",
                        "<cell>18: \u001b[34mnote:\u001b[m      Subclass:\u001b[m\n",
                        "<cell>18: \u001b[34mnote:\u001b[m          def get_processor_run_args(self, shared_config: int) -> dict[Any, Any]\u001b[m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Successfully ran cell\n"
                    ]
                }
            ],
            "source": [
                "# Use\n",
                "# ====\n",
                "class FrameworkProcessingStepFactoryInterface(ProcessingStepFactoryInterface):\n",
                "    def __init__(self):\n",
                "        pass\n",
                "\n",
                "    # This is not ok - return type is not of (sub)type `ProcessingStep``. Mypy catches this by default.\n",
                "    def create_step(self, shared_config) -> None:\n",
                "        return None\n",
                "\n",
                "    # This is not okay - return type is not of (sub)type `Processor`.\n",
                "    # However, mypy doesn't catch it by default - but it does here since I manually enabled type\n",
                "    # checking for Sagemaker-sdk.\n",
                "    def processor(self) -> int:\n",
                "        return 0\n",
                "\n",
                "    # This is not ok - argument type is not of (super)type `SharedConfig``. Mypy catches this by default.\n",
                "    def get_processor_run_args(self, shared_config: int) -> dict:\n",
                "        return {}\n",
                "\n",
                "    # This is ok\n",
                "    def get_processor_extra_kwargs(self, shared_config: SharedConfig) -> dict:\n",
                "        return {}\n",
                "\n",
                "\n",
                "framework_processing_step = FrameworkProcessingStepFactoryInterface()\n",
                "framework_processing_step.processor()\n",
                "print('\\nSuccessfully ran cell')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mypy does accept type generalizations following the Liskov Substitution Principle\n",
                "We are able to use more general or specific types in our implementation if they follow the Liskov Substitution Principle: Methods can *accept more general types* as arguments and *return a more specific types*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<cell>20: \u001b[1m\u001b[31merror:\u001b[m Signature of \u001b[m\u001b[1m\"get_processor_run_args\"\u001b[m incompatible with supertype \u001b[m\u001b[1m\"ProcessingStepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[override]\u001b[m\n",
                        "<cell>20: \u001b[34mnote:\u001b[m      Superclass:\u001b[m\n",
                        "<cell>20: \u001b[34mnote:\u001b[m          def get_processor_run_args(self) -> dict[Any, Any]\u001b[m\n",
                        "<cell>20: \u001b[34mnote:\u001b[m      Subclass:\u001b[m\n",
                        "<cell>20: \u001b[34mnote:\u001b[m          def get_processor_run_args(self, shared_config: SharedConfig | dict[Any, Any]) -> dict[Any, Any]\u001b[m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
                        "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n",
                        "\n",
                        "Successfully ran cell\n"
                    ]
                }
            ],
            "source": [
                "# Use\n",
                "# ====\n",
                "class FrameworkProcessingStepFactoryInterface(ProcessingStepFactoryInterface):\n",
                "    def __init__(self):\n",
                "        pass\n",
                "\n",
                "    def create_step(self, shared_config: SharedConfig) -> ProcessingStep:\n",
                "        return ProcessingStep(name='preprocessing')\n",
                "\n",
                "    # This is okay: *Return* more *specific* type.\n",
                "    def processor(self) -> FrameworkProcessor:\n",
                "        return FrameworkProcessor(\n",
                "            estimator_cls=SKLearn,\n",
                "            framework_version='0.23-1',\n",
                "            role='dummy_role',\n",
                "            instance_type='ml.m5.xlarge',\n",
                "        )\n",
                "\n",
                "    # This is ok: *Accept* more *general* argument type.\n",
                "    def get_processor_run_args(self, shared_config: SharedConfig | dict) -> dict:\n",
                "        return {}\n",
                "\n",
                "    def get_processor_extra_kwargs(self, shared_config: SharedConfig) -> dict:\n",
                "        return {}\n",
                "\n",
                "\n",
                "framework_processing_step = FrameworkProcessingStepFactoryInterface()\n",
                "framework_processing_step.processor()\n",
                "print('\\nSuccessfully ran cell')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to incorporate need for different configs?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Args\n",
                "# ========\n",
                "class ProcessorRunArgs(TypedDict):\n",
                "    inputs: list[ProcessingInput]\n",
                "    outputs: list[ProcessingOutput]\n",
                "    arguments: list[str] | None\n",
                "\n",
                "class FrameworkProcessorRunArgs(ProcessorRunArgs):\n",
                "    # Additional args for FrameworkProcessor:\n",
                "    source_dir: str\n",
                "    code: str\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Redefining interfaces without shared config, remove method to get extra kwargs\n",
                "from sagemaker.workflow.steps import ProcessingStep\n",
                "\n",
                "\n",
                "class BaseStepFactoryInterface(ABC):\n",
                "    @abstractmethod\n",
                "    def create_step(self) -> ConfigurableRetryStep:\n",
                "        ...\n",
                "class ProcessingStepFactoryInterface(BaseStepFactoryInterface):\n",
                "    @abstractmethod\n",
                "    def create_step(self) -> ProcessingStep:\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def processor(self) -> Processor:\n",
                "        ...\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_processor_run_args(self) -> ProcessorRunArgs:\n",
                "        ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Different Configs\n",
                "# =================\n",
                "class StepConfig(BaseSettings):\n",
                "    input_filename: str\n",
                "    output_filename: str\n",
                "    output_train_filename: str\n",
                "    output_val_filename: str\n",
                "    output_test_filename: str\n",
                "    instance_type: str\n",
                "    instance_count: int\n",
                "    step_name: str\n",
                "\n",
                "class ProcessingConfig(StepConfig):\n",
                "    \"\"\"\n",
                "    This class provides the schema for the step-specific config file.\n",
                "    It is passed to step factory in the latter's constructor.\n",
                "    \"\"\"\n",
                "    sklearn_framework_version: str\n",
                "    # Override default field with more specific filenames\n",
                "    output_filename: None = None  # type: ignore[assignment]\n",
                "    output_train_filename: str\n",
                "    output_val_filename: str\n",
                "    output_test_filename: str\n",
                "\n",
                "# class FrameworkProcessingConfig(ProcessingConfig):\n",
                "#     \"\"\"\n",
                "#     So far no extra configs needed. (While it would be nice if we could set `estimator_cls=SKLearn`\n",
                "#     in the config file, but we would have to use `eval()` to construct a python object from the\n",
                "#     string, which is a potential security vulnerability.)\n",
                "#     \"\"\"\n",
                "#     ...\n",
                "\n",
                "# processing_config: FrameworkProcessingConfig = load_pydantic_config_from_file(  # type: ignore\n",
                "#     config_cls=FrameworkProcessingConfig,\n",
                "#     config_path=\"sm_pipelines_oo/configs/dev/.env_pre_process\",\n",
                "# )\n",
                "# print(processing_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Literal\n",
                "\n",
                "class FrameworkProcessingConfig(BaseSettings):\n",
                "    estimator_cls: Literal['SKLearn']  # todo: add other available estimators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<cell>42: \u001b[1m\u001b[31merror:\u001b[m Return type \u001b[m\u001b[1m\"ProcessorRunArgs\"\u001b[m of \u001b[m\u001b[1m\"get_processor_run_args\"\u001b[m incompatible with return type \u001b[m\u001b[1m\"dict[Any, Any]\"\u001b[m in supertype \u001b[m\u001b[1m\"ProcessingStepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[override]\u001b[m\n",
                        "<cell>83: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"Callable[[], FrameworkProcessor]\"\u001b[m has no attribute \u001b[m\u001b[1m\"run\"\u001b[m  \u001b[m\u001b[33m[attr-defined]\u001b[m\n"
                    ]
                }
            ],
            "source": [
                "from functools import cached_property\n",
                "\n",
                "# Realistic implementation\n",
                "# ========================\n",
                "class FrameworkProcessingStepFactory(ProcessingStepFactoryInterface):\n",
                "    def __init__(\n",
                "        self,\n",
                "        processing_config: ProcessingConfig,  # subtype uses more *specific arg* type\n",
                "        shared_config: SharedConfig,\n",
                "        fw_processing_config: FrameworkProcessingConfig,\n",
                "        aws_connector: AWSConnectorInterface,\n",
                "    ):\n",
                "        self.aws_connector = aws_connector\n",
                "        self.step_config = processing_config\n",
                "        self.shared_config = shared_config\n",
                "        self.frameworkprocessor_config = fw_processing_config\n",
                "\n",
                "        # This determines how to construct the estimator object from the string in the config file, avoiding the\n",
                "        # use of `eval`, which is a potential security vulnerability.\n",
                "        self._str_to_cls_mapping: dict[str, Any] = {  # todo:  find supertype\n",
                "            'SKLearn': SKLearn,\n",
                "        }\n",
                "\n",
                "    def support_additional_estimators(self, additional_estimator_mapping: dict[str, Any]) -> None:\n",
                "        \"\"\"Allow user to add additional estimators (following the open-closed principle).\"\"\"\n",
                "        self._str_to_cls_mapping.update(additional_estimator_mapping)\n",
                "\n",
                "\n",
                "    @cached_property\n",
                "    def _estimator_cls(self):\n",
                "        class_as_string = self.frameworkprocessor_config.estimator_cls\n",
                "        return self._str_to_cls_mapping[class_as_string]\n",
                "\n",
                "    @cached_property\n",
                "    def _path_factory(self) -> PathFactory:\n",
                "        return PathFactory(\n",
                "            # todo: separate general, step, and specific configs\n",
                "            step_config=self.step_config,  # type: ignore\n",
                "            shared_config=self.shared_config\n",
                "        )\n",
                "\n",
                "    def get_processor_run_args(self) -> FrameworkProcessorRunArgs:\n",
                "        s3_input_folder: str = self._path_factory.s3_input_folder\n",
                "        s3_output_folder: str = self._path_factory.s3_output_folder\n",
                "        local_folderpath: str = self._path_factory.local_folderpath\n",
                "\n",
                "        skl_run_args = FrameworkProcessorRunArgs(\n",
                "            inputs = [\n",
                "                ProcessingInput(\n",
                "                    source=s3_input_folder,\n",
                "                    destination=f\"{local_folderpath}/input/\"\n",
                "                ),\n",
                "            ],\n",
                "            outputs = [\n",
                "                ProcessingOutput(\n",
                "                    output_name=\"train\",\n",
                "                    source=f\"/{local_folderpath}/train\",\n",
                "                    destination=f\"{s3_output_folder}/train\",\n",
                "                ),\n",
                "                ProcessingOutput(\n",
                "                    output_name=\"validation\",\n",
                "                    source=f\"/{local_folderpath}/validation\",\n",
                "                    destination=f\"{s3_output_folder}/validation\",\n",
                "                ),\n",
                "                ProcessingOutput(\n",
                "                    output_name=\"test\",\n",
                "                    source=f\"/{local_folderpath}/test\",\n",
                "                    destination=f\"{s3_output_folder}/test\",\n",
                "                ),\n",
                "            ],\n",
                "            source_dir=self._path_factory.source_dir,\n",
                "            code=self._path_factory.step_code_file,\n",
                "            arguments=None # Todo: Decide whether this should come from configuration. May depend on type of step.\n",
                "        )\n",
                "        return skl_run_args\n",
                "\n",
                "    def create_step(self) -> ProcessingStep:\n",
                "        \"\"\"\n",
                "        Note that this can only be run from the PipelineWrapper, because this factory does not have\n",
                "        access to the shared configs.\n",
                "        \"\"\"\n",
                "        run_args: ProcessorRunArgs = self.get_processor_run_args()\n",
                "        step_args: _JobStepArguments = self.processor.run(**run_args)\n",
                "        return ProcessingStep(\n",
                "            name=self.step_config.step_name,\n",
                "            step_args=step_args,  # type: ignore\n",
                "        )\n",
                "\n",
                "    def processor(self) -> FrameworkProcessor:\n",
                "        return FrameworkProcessor(\n",
                "            framework_version=self.step_config.sklearn_framework_version,\n",
                "            instance_type=self.step_config.instance_type,\n",
                "            instance_count=self.step_config.instance_count,\n",
                "            base_job_name=self.step_config.step_name,\n",
                "            sagemaker_session=self.aws_connector.sm_session,\n",
                "            role=self.aws_connector.role_arn,\n",
                "            estimator_cls=self._estimator_cls,\n",
                "        )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sm_pipelines_oo.aws_connector.implementation import create_aws_connector\n",
                "\n",
                "\n",
                "class SharedConfig(BaseSettings):  # type: ignore\n",
                "    \"\"\"Defines configuration shared by all pipeline steps (for a given environment).\"\"\"\n",
                "    project_name: str\n",
                "    project_version: str  # Versions data (and probably more in the future)\n",
                "    region: str\n",
                "    # To do: consider which of these fields should be made required.\n",
                "    role_name: str | None = None\n",
                "    project_bucket_name: str\n",
                "\n",
                "shared_config: SharedConfig = SharedConfig(\n",
                "    project_name='design-decisions',\n",
                "    project_version='0.0',  # Versions data (and probably more in the future)\n",
                "    region='us-east-1',\n",
                "    # To do: consider which of these fields should be made required.\n",
                "    role_name=None,\n",
                "    project_bucket_name='design-decisions',\n",
                ")\n",
                "\n",
                "class ProcessingConfig(BaseSettings):  # type: ignore\n",
                "    input_filename: str\n",
                "    instance_type: str\n",
                "    instance_count: int\n",
                "    sklearn_framework_version: str\n",
                "    # Don't set in config. This needs to correspond to SM's convention for local folder structure.\n",
                "    # todo: Make this not set-able. Use property instead?\n",
                "    step_type: Literal['processing'] = \"processing\"\n",
                "    step_name: str = \"processing\"\n",
                "\n",
                "processing_config = ProcessingConfig(\n",
                "    input_filename='input.parquet',\n",
                "    instance_type='local',\n",
                "    instance_count=1,\n",
                "    sklearn_framework_version='0.23-1',\n",
                ")\n",
                "\n",
                "aws_connector: AWSConnectorInterface = create_aws_connector(\n",
                "    environment='dev',\n",
                "    shared_config=shared_config,\n",
                "    run_as_pipeline=True,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
                        "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2023-12-24 13:30:51.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msm_pipelines_oo.aws_connector.implementation\u001b[0m:\u001b[36mrole_arn\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mrole: arn:aws:iam::338755209567:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AdministratorAccess_7b40736629c71dd9\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
                        "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n",
                        "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
                        "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n",
                        "\n",
                        "Successfully ran cell\n"
                    ]
                }
            ],
            "source": [
                "framework_processing_step = FrameworkProcessingStepFactory(\n",
                "    # processor_cls=FrameworkProcessor,\n",
                "    processing_config=processing_config,\n",
                "    fw_processing_config=FrameworkProcessingConfig(estimator_cls='SKLearn'),\n",
                "    shared_config=shared_config,\n",
                "    aws_connector=aws_connector,\n",
                ")\n",
                "\n",
                "framework_processing_step.processor()\n",
                "print('\\nSuccessfully ran cell')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from functools import cached_property\n",
                "from datetime import datetime\n",
                "\n",
                "from loguru import logger\n",
                "from sagemaker.workflow.pipeline import Pipeline\n",
                "from sagemaker.workflow.steps import Step\n",
                "\n",
                "from sm_pipelines_oo.shared_config_schema import SharedConfig, Environment\n",
                "from sm_pipelines_oo.steps.interfaces import StepFactoryInterface\n",
                "from sm_pipelines_oo.aws_connector.interface import AWSConnectorInterface\n",
                "\n",
                "\n",
                "class PipelineWrapper:\n",
                "    \"\"\"\n",
                "    Holds configs & code\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        step_factories: dict[StepFactoryInterface, Any],\n",
                "        environment: Environment,\n",
                "        shared_config: SharedConfig,\n",
                "    ) -> None:\n",
                "        self.environment = environment\n",
                "        self.shared_config = shared_config\n",
                "\n",
                "        # Other setup\n",
                "        self.steps: list[Step] = []\n",
                "        self._create_steps(step_factories, shared_config)\n",
                "        self._aws_connector: AWSConnectorInterface = create_aws_connector(\n",
                "            environment=environment,\n",
                "            shared_config=shared_config,\n",
                "            run_as_pipeline=True,\n",
                "        )\n",
                "\n",
                "\n",
                "    def _create_steps(self, step_factories: list[StepFactoryInterface], shared_config: SharedConfig) -> None:\n",
                "        for factory, configs in step_factories.items():\n",
                "            step: Step = factory.create_step(**configs)\n",
                "            self.steps.append(step)\n",
                "\n",
                "    @cached_property\n",
                "    def _pipeline(self) -> Pipeline:\n",
                "        pipeline_name = f'{self.shared_config.project_name}-{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
                "        pipeline = Pipeline(\n",
                "            name=pipeline_name,\n",
                "            steps=self.steps,\n",
                "            sagemaker_session=self._aws_connector.sm_session,\n",
                "        )\n",
                "        pipeline.create(role_arn=self._aws_connector.role_arn)\n",
                "        return pipeline\n",
                "\n",
                "\n",
                "    # Public methods\n",
                "    # ==============\n",
                "\n",
                "    def run(self) -> None:\n",
                "        logger.info(f\"Starting pipeline run for project {self.shared_config.project_name}\")\n",
                "        execution = self._pipeline.start()\n",
                "        execution.wait()\n",
                "        execution.list_steps()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processing_config = ProcessingConfig(\n",
                "    input_filename='input.parquet',\n",
                "    instance_type='local',\n",
                "    instance_count=1,\n",
                "    sklearn_framework_version='0.23-1',\n",
                ")\n",
                "fw_processing_config = FrameworkProcessingConfig(estimator_cls='SKLearn')\n",
                "\n",
                "pipeline = PipelineWrapper(\n",
                "    step_factories={\n",
                "        FrameworkProcessingStepFactory: [processing_config, fw_processing_config],\n",
                "    },\n",
                "    environment=ENVIRONMENT,\n",
                "    shared_config=shared_config,\n",
                ")\n",
                "try:\n",
                "    pipeline.run()\n",
                "except Exception as e:\n",
                "    logger.error(e)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sm-pipelines-oo-tWfBw0_D-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}