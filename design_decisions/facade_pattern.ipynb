{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 1.0.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make imports and folder paths work\n",
    "# todo: Instead create python package and install locally\n",
    "import os, sys\n",
    "os.chdir(\n",
    "    f'{os.environ[\"HOME\"]}/repos/sagemaker-pipelines-abstraction/src'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from functools import cached_property\n",
    "from typing import Literal, Callable, TypeAlias\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import TypeVar, Generic\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "from loguru import logger\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import Step\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.workflow.steps import ConfigurableRetryStep, ProcessingStep\n",
    "from sm_pipelines_oo.shared_config_schema import Environment\n",
    "\n",
    "from sm_pipelines_oo.shared_config_schema import SharedConfig, Environment\n",
    "# from sm_pipelines_oo.steps.interfaces import StepFactoryInterface\n",
    "from sm_pipelines_oo.connector.interface import AWSConnectorInterface\n",
    "from sm_pipelines_oo.utils import load_pydantic_config_from_file\n",
    "from sm_pipelines_oo.connector.interface import AWSConnectorInterface\n",
    "from sm_pipelines_oo.connector.implementation import create_aws_connector\n",
    "from sm_pipelines_oo.pipeline_wrapper import PipelineWrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facade + Factory-Patterns\n",
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config classes for varying level of generality\n",
    "# ==============================================\n",
    "\n",
    "class StepConfig(BaseSettings):\n",
    "    input_filename: str\n",
    "    output_filename: str\n",
    "    output_train_filename: str\n",
    "    output_val_filename: str\n",
    "    output_test_filename: str\n",
    "    instance_type: str\n",
    "    instance_count: int\n",
    "    step_name: str\n",
    "\n",
    "class ProcessingConfig(BaseSettings):\n",
    "    \"\"\"\n",
    "    This class provides the schema for the step-specific config file.\n",
    "    It is passed to step factory in the latter's constructor.\n",
    "    \"\"\"\n",
    "    sklearn_framework_version: str\n",
    "    # Override default field with more specific filenames\n",
    "    output_filename: None = None  # type: ignore[assignment]\n",
    "    output_train_filename: str\n",
    "    output_val_filename: str\n",
    "    output_test_filename: str\n",
    "\n",
    "class FrameworkProcessingConfig(BaseSettings):\n",
    "    \"\"\"\n",
    "    So far no extra configs needed. (While it would be nice if we could set `estimator_cls=SKLearn`\n",
    "    in the config file, but we would have to use `eval()` to construct a python object from the\n",
    "    string, which is a potential security vulnerability.)\n",
    "    \"\"\"\n",
    "    estimator_cls: Literal['SKLearn'] = 'SKLearn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Config Facade\n",
    "# =====================\n",
    "\n",
    "# Each of the types must be a subclass of BaseSettings\n",
    "StepSpecificConfigType = TypeVar(\"StepSpecificConfigType\", bound=BaseSettings)\n",
    "AdditionalConfigType = TypeVar(\"AdditionalConfigType\", bound=BaseSettings)\n",
    "\n",
    "@dataclass\n",
    "class StepConfigFacade(\n",
    "    Generic[StepSpecificConfigType, AdditionalConfigType]\n",
    "):\n",
    "    # This config type is hard-coded, since it does not depend on step type.\n",
    "    general_step_config: StepConfig\n",
    "    step_specific_config: StepSpecificConfigType\n",
    "    additional_config: AdditionalConfigType\n",
    "\n",
    "FrameworkProcessingConfigFacade = StepConfigFacade[\n",
    "    ProcessingConfig,\n",
    "    FrameworkProcessingConfig,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example configs\n",
    "_step_config = StepConfig(\n",
    "    input_filename='input.parquet',\n",
    "    output_filename='output.parquet',\n",
    "    output_train_filename='output_train.parquet',\n",
    "    output_val_filename='output_val.parquet',\n",
    "    output_test_filename='output_test.parquet',\n",
    "    instance_type='local',\n",
    "    instance_count=1,\n",
    "    step_name='processing',\n",
    ")\n",
    "\n",
    "_processing_config = ProcessingConfig(\n",
    "    sklearn_framework_version='0.23-1',\n",
    "    output_train_filename='output_train.parquet',\n",
    "    output_val_filename='output_val.parquet',\n",
    "    output_test_filename='output_test.parquet',\n",
    ")\n",
    "\n",
    "_framework_processing_config = FrameworkProcessingConfig(\n",
    "    estimator_cls='SKLearn',\n",
    ")\n",
    "\n",
    "fw_proc_configs = FrameworkProcessingConfigFacade(\n",
    "    general_step_config=_step_config,\n",
    "    step_specific_config=_processing_config,\n",
    "    additional_config=_framework_processing_config,\n",
    ")\n",
    "\n",
    "def get_mock_fwp_configs() -> FrameworkProcessingConfigFacade:\n",
    "    # this is for testing\n",
    "    return fw_proc_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *General* step factory interface\n",
    "# ==============================\n",
    "class StepFactoryInterface(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_step(\n",
    "         self,\n",
    "         shared_config: SharedConfig,\n",
    "         step_configs: StepConfigFacade\n",
    "    ) -> ConfigurableRetryStep:\n",
    "        ...\n",
    "    # @property\n",
    "    # @abstractmethod\n",
    "    # def step_name(self) -> str:\n",
    "    #     ...\n",
    "    # @abstractmethod\n",
    "    # def _get_step_config(self, environment: Environment) -> dict:  # todo: improve type\n",
    "    #     ...\n",
    "\n",
    "\n",
    "class ProcessingStepFactoryInterface(StepFactoryInterface):\n",
    "    \"\"\"This subclass is distinguished only by more specific return type for step.\"\"\"\n",
    "    @abstractmethod\n",
    "    def create_step(\n",
    "        self,\n",
    "        shared_config: SharedConfig,\n",
    "        step_configs: StepConfigFacade\n",
    "    ) -> ProcessingStep:\n",
    "         ...\n",
    "\n",
    "class FrameworkProcessorFactory(ProcessingStepFactoryInterface):\n",
    "    def __init__(\n",
    "            self,\n",
    "            step_name: str,  # identifies step config (given env)\n",
    "        ) -> None:\n",
    "            self.step_name = step_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProcessingStepFactoryInterface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ==============\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFrameworkProcessingStepFactory\u001b[39;00m(\u001b[43mProcessingStepFactoryInterface\u001b[49m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    shared config etc will be passed during create_step().\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     12\u001b[0m         env: Environment, \u001b[38;5;66;03m# Identifies path to configs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         step_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;66;03m# Identifies path to configs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ProcessingStepFactoryInterface' is not defined"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "# ==============\n",
    "from typing import Any\n",
    "\n",
    "class FrameworkProcessingStepFactory(ProcessingStepFactoryInterface):\n",
    "    \"\"\"\n",
    "    shared config etc will be passed during create_step().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Environment, # Identifies path to configs\n",
    "        step_name: str, # Identifies path to configs\n",
    "    ):\n",
    "        self._step_name = step_name\n",
    "        self._env = env\n",
    "\n",
    "        # This determines how to construct the estimator object from the string in the config file, avoiding the\n",
    "        # use of `eval`, which is a potential security vulnerability.\n",
    "        self._str_to_cls_mapping: dict[str, Any] = {  # todo:  find supertype\n",
    "            'SKLearn': SKLearn,\n",
    "        }\n",
    "        self._step_config: FrameworkProcessingConfigFacade = self._construct_step_configs(\n",
    "            env=env,\n",
    "            step_name=step_name,\n",
    "        )\n",
    "\n",
    "    def support_additional_estimators(self, additional_estimator_mapping: dict[str, Any]) -> None:\n",
    "        \"\"\"Allow user to add additional estimators (following the open-closed principle).\"\"\"\n",
    "        self._str_to_cls_mapping.update(additional_estimator_mapping)\n",
    "\n",
    "    def _construct_step_configs(self, env: Environment, step_name: str) -> FrameworkProcessingConfigFacade:\n",
    "        \"\"\"Load configs from file and return them as a single wrapper object.\"\"\"\n",
    "        # todo: get real configs, once completed testing, and create separate test facade.\n",
    "        return get_mock_fwp_configs()\n",
    "\n",
    "\n",
    "    # todo: Generalize types to other processors\n",
    "    def _processor(self, step_configs: FrameworkProcessingConfig) -> Processor:  # type: ignore\n",
    "        \"\"\"\n",
    "        Instantiate processor, combining step-specific configs with configs from AWS connector.\n",
    "\n",
    "        Note that we could technically run this in __init__() now, because we do no longer use\n",
    "        anything from the shared_config. However, leaving it here keeps the option open to make it\n",
    "        a separate method that accepts outside configs as arguments, if necessary in the future.\n",
    "        \"\"\"\n",
    "        return self._processor_cls(\n",
    "            framework_version=step_configs.sklearn_framework_version,\n",
    "            instance_type=step_configs.instance_type,\n",
    "            instance_count=step_configs.instance_count,\n",
    "            base_job_name=step_configs.step_name,\n",
    "            sagemaker_session=self.aws_connector.sm_session,\n",
    "            role=self.aws_connector.role_arn,\n",
    "            **self._processor_extra_kwargs,\n",
    "        )  # type: ignore\n",
    "\n",
    "    def _get_processor_run_args(self) -> ProcessorRunArgs:\n",
    "        s3_input_folder: str = self.path_factory.s3_input_folder\n",
    "        s3_output_folder: str = self.path_factory.s3_output_folder\n",
    "        local_folderpath: str = self.path_factory.local_folderpath\n",
    "\n",
    "        skl_run_args = ProcessorRunArgs(\n",
    "            inputs = [\n",
    "                ProcessingInput(\n",
    "                    source=s3_input_folder,\n",
    "                    destination=f\"{local_folderpath}/input/\"\n",
    "                ),\n",
    "            ],\n",
    "            outputs = [\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"train\",\n",
    "                    source=f\"/{local_folderpath}/train\",\n",
    "                    destination=f\"{s3_output_folder}/train\",\n",
    "                ),\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"validation\",\n",
    "                    source=f\"/{local_folderpath}/validation\",\n",
    "                    destination=f\"{s3_output_folder}/validation\",\n",
    "                ),\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"test\",\n",
    "                    source=f\"/{local_folderpath}/test\",\n",
    "                    destination=f\"{s3_output_folder}/test\",\n",
    "                ),\n",
    "            ],\n",
    "            source_dir=self.path_factory.source_dir,\n",
    "            code=self.path_factory.step_code_file,\n",
    "            arguments=None # Todo: Decide whether this should come from configuration. May depend on type of step.\n",
    "        )\n",
    "        return skl_run_args\n",
    "\n",
    "    # todo: Add more specific return type (may have to create custom type, but check Sagemaker sdk code again)\n",
    "    def create_step(self, env: Environment, shared_config: SharedConfig) -> ProcessingStep:\n",
    "        # todo: think about how to create these here\n",
    "        step_configs = fw_proc_configs\n",
    "\n",
    "        return ProcessingStep(\n",
    "            name=step_config.step_name,\n",
    "            processor=FrameworkProcessor(\n",
    "                estimator_cls=step_configs,\n",
    "                framework_version=step_config.sklearn_framework_version,\n",
    "                instance_type=step_config.instance_type,\n",
    "                instance_count=step_config.instance_count,\n",
    "                role=step_config.role_name,\n",
    "            ),\n",
    "            inputs=[ProcessingInput(\n",
    "                source=step_config.input_data,\n",
    "                destination=step_config.output_data,\n",
    "                s3_data_type='S3Prefix',\n",
    "                s3_input_mode='File',\n",
    "            )],\n",
    "            outputs=[ProcessingOutput(\n",
    "                source=step_config.output_data,\n",
    "                destination=step_config.output_data,\n",
    "                s3_upload_mode='EndOfJob',\n",
    "            )],\n",
    "            code=step_config.code,\n",
    "        )\n",
    "fw_proc_step_factory = FrameworkProcessingStepFactory(step_name='preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineFacade:\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_factories: list[StepFactoryInterface],\n",
    "        env: Environment,\n",
    "        shared_config: SharedConfig,\n",
    "    ) -> None:\n",
    "        self._env: Environment = env\n",
    "        self._shared_config = shared_config\n",
    "        self._step_configs = self._get_step_configs(env)\n",
    "        self._step_factories = step_factories\n",
    "\n",
    "    @cached_property\n",
    "    def _steps(self) -> list[Step]:\n",
    "        steps: list[Step] = []\n",
    "        for factory in self._step_factories:\n",
    "            step: Step = factory.create_step(\n",
    "                shared_config=self._shared_configs,\n",
    "                step_config=self._step_configs,\n",
    "            )\n",
    "            steps.append(step)\n",
    "        return steps\n",
    "\n",
    "    @cached_property\n",
    "    def _aws_connector(self) -> AWSConnectorInterface:\n",
    "        \"\"\"\n",
    "        This code makes connector.implementation.create_aws_connector() redundant, except for use\n",
    "        outside of pipeline.\n",
    "        Todo: decide where to put code for the latter case.\n",
    "        \"\"\"\n",
    "        if self._env == 'local':\n",
    "            return LocalAWSConnector()\n",
    "        else:\n",
    "            return AWSConnector(\n",
    "                environment=self._env,\n",
    "                # this error will resolve once we don't use SharedConfig from this notebook but\n",
    "                # library's AWSConnector.\n",
    "                shared_config=self._shared_config,  # type: ignore\n",
    "                run_as_pipeline=True\n",
    "            )\n",
    "\n",
    "    @cached_property\n",
    "    def _pipeline(self) -> Pipeline:\n",
    "        \"\"\"\n",
    "        We could make this a private  method and call it in __init__(), but this is shorter.\n",
    "        \"\"\"\n",
    "        pipeline_name = f'{self._shared_config.project_name}-{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "        pipeline = Pipeline(\n",
    "            name=pipeline_name,\n",
    "            steps=self.steps,\n",
    "            sagemaker_session=self._aws_connector.sm_session,\n",
    "        )\n",
    "        pipeline.create(role_arn=self._aws_connector.role_arn)\n",
    "        return pipeline\n",
    "\n",
    "    def run(self) -> None:\n",
    "        try:\n",
    "            logger.info(f\"Starting pipeline run for project {self._shared_config.project_name}\")\n",
    "            execution = self._pipeline.start()\n",
    "            execution.wait()\n",
    "            execution.list_steps()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm-pipelines-oo-tWfBw0_D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
