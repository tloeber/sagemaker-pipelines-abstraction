{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old explanation of design (overhaul/integrate):\n",
    "\n",
    "By contrast to earlier designs, create_step does not require a step_config to be passed here as an argument. This simplify the interface considerably because we don't have to worry about getting the right type of step_config for a given step_type. The problem is that while we can simply *return more specific* step types in subclasses (such as a ProcessingStep instead of a ConfigurableRetryStep), we can**not** require a *more specific argument* for subclasses (for example requiring a ProcessingStepConfig for a ProcessingStepFactory), as this would violate the Liskov Substitution Principal.\n",
    "\n",
    "This problem would not even be easily solved by using generics, because it is not obvious how we can go from my given Steptype to the associated StepConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 1.0.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from typing import  ClassVar, TypeVar, TypeAlias, Any, final, TypedDict\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "from sagemaker.processing import Processor\n",
    "from sagemaker.estimator import EstimatorBase\n",
    "\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep, \\\n",
    "    TuningStep, ConfigurableRetryStep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why no generics?\n",
    "- The end goal is simply to have an object that satisfies the (ConfigurableRetry)StepInterface. From the perspective of the pipeline, we don't care what type of step it is.\n",
    "- The initial reason for looking into leveraging generics was for making sure that we are passing the right config for a given type of step. However, after a lot of trial and error, I still did not find a good way to create a simple class hierarchy based on what the Sagemaker SDK makes available to us. Instead, it looks more promising to simply create a very minimal interface for step factories, and later specific implementations decide what the best way to create that kind of step is.\n",
    "  - Downside: let's reuse of code between different step factories. This makes it somewhat harder to get started with creating new step factories, because there is less structure imposed for how exactly to do it.\n",
    "  - Upside: more flexibility for grading step factories. This may actually make it easier to create new step factories, and it will make it easier to maintain given step factories as the interface of the Sagemaker SDK changes.\n",
    "  - Note: neither of these points will affect a basic library user who only uses inbuilt step factories.\n",
    "\n",
    "# Configuration\n",
    "Goal:  Abstract configuration into a single config class which loads all config's it needs in the directory (even if this requires traversing). This will not only make the intent of this method more clear, but it will also make it easier to have a single config façade that abstracts what config's are global and what are step-specific (step config simply need a reference to the shared config, so they can fall back to that if necessary, but the concrete logic can be implemented differently for each step type). Also, having a config façade makes it easy to define methods that compute derived values.\n",
    "\n",
    "## Attempt 1: *Overarching* ConfigFacade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConfigFacade:\n",
    "#     def __init__(self, config_dir: Path):\n",
    "#         # load all the yaml config files\n",
    "#         shared_config_dict: dict[str, Any] = ...\n",
    "#         step_configs_dicts: dict[str, dict[str, Any]] = ...\n",
    "\n",
    "#         # Convert the dictionaries to pydantic models\n",
    "#         self.shared_config: SharedConfig = SharedConfig(**shared_config_dict)\n",
    "#         self.step_configs: dict[str, BaseSettings] = {}\n",
    "#         for step_name, step_config_dict  in step_configs_dicts.items():\n",
    "\n",
    "#             self.step_configs[step_name] = StepConfig(**step_config_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: how to get  the pydantic model for a given config? While it would be possible to have another look up table, similar to how we find the right specific step factory, it makes more sense that each step factory owns the associated config model. This is because the key challenge is that the  config model matches the specific factory. \n",
    "\n",
    "As a result, it  is better to not load all the configs upfront (except possibly into dictionaries).\n",
    "\n",
    "## Attempt 2: *Separate* Configs w/o facade, but *reference* to shared config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "\n",
    "class SharedConfigInterface(BaseSettings):\n",
    "    \"\"\"\n",
    "    This interface defines all the configs that our library code expects to be present in the shared_config.\n",
    "    \"\"\"\n",
    "    project_name: str\n",
    "    project_version: str  # Versions data (and probably more in the future)\n",
    "\n",
    "\n",
    "# class StepConfigInterface(BaseSettings):\n",
    "#     \"\"\"\n",
    "#     This ensures every step config has a step_type (required to determine step factory),  as well as a reference to the shared_config.\n",
    "\n",
    "#     Note: If the concrete step_config depends on any specific config values being set in the shared_config (in addition to the ones defined in the SharedConfigInterface), we should redefine the type of shared_config to this more specific type.\n",
    "#     \"\"\"\n",
    "#     step_type: str # Identifies factory, which in turn identifies StepConfig model\n",
    "#     shared_config: SharedConfigInterface # So that we have access to sharedconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple factory\n",
    "~~This makes better use of factory, because depending on the arg passed to it, it creates a different type of step. Otherwise, we may as well us strategy pattern (only use of factory is to construct step later when configs etc are known - but a given factory always produces same kind of step, except from configuration).~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepFactoryInterface(ABC):\n",
    "    \"\"\"\n",
    "    In addition to the required methods defined below, it is recommended to implement the following attributes and methods in order to make implementation of the required methods easiest:\n",
    "    - _config_model: ClassVar[type[BaseSettings]] (Class used to convert config_dict to pydantic model to validate types and potentially compute derived attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, step_config_dict: dict[str, Any]):\n",
    "        ...\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # @abstractmethod\n",
    "    # def _get_config_model() -> type[BaseSettings]:\n",
    "    #     \"\"\"\n",
    "    #     Pydantic model used to validate and convert the config_dict to an instance of pydantic.BaseSettings.\n",
    "    #     \"\"\"\n",
    "    #     ...\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_step(self) -> ConfigurableRetryStep:\n",
    "        # Note that we don't have to worry about violating the LSP -  even though we are adding back an argument for the config – because at this stage that config will simply be of type dictionary. Thus, subclasses don't have to specify a more specific subtype of config here yet.\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python < 3.12, don't use typing.TypedDict: https://docs.pydantic.dev/2.6/errors/usage_errors/#typed-dict-version\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.estimator import EstimatorBase\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "class _FWProcessorInitConfig(TypedDict):\n",
    "    framework_version: str\n",
    "    estimator_cls_name: str\n",
    "    instance_count: int\n",
    "    instance_type: str\n",
    "\n",
    "\n",
    "class _FWProcessorRunConfig(TypedDict):\n",
    "    code: str\n",
    "    source_dir: str\n",
    "    # todo: allow athena datasetdefinition instead\n",
    "    input_files_s3paths: list[str]  # todo: validate it's an s3 path\n",
    "    output_files_s3paths: list[str]  # todo: validate it's an s3 path\n",
    "\n",
    "\n",
    "class FrameworkProcessingStepConfig(BaseSettings):\n",
    "    # todo:\n",
    "    step_name: str\n",
    "    processor_init_args: _FWProcessorInitConfig\n",
    "    processor_run_args: _FWProcessorRunConfig\n",
    "    # For now, we will reload this for every step config to avoid dependency on pipeline wrapper.\n",
    "    shared_config: SharedConfigInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "\n",
    "class FrameworkProcessingStepFactory(StepFactoryInterface):\n",
    "    # Note: this is a public attribute, so user can add support for additional estimators\n",
    "    estimator_name_to_cls_mapping: ClassVar[dict[str, Any]] = {  # todo:  find supertype\n",
    "        'SKLearn': SKLearn,\n",
    "    }\n",
    "\n",
    "    _config_model: ClassVar[type[FrameworkProcessingStepConfig]] = FrameworkProcessingStepConfig\n",
    "\n",
    "    def __init__(self, step_config_dict: dict[str, Any]):\n",
    "        # Parse config, using the specific pydantic model that this factory has as a class variable.\n",
    "        self._config: FrameworkProcessingStepConfig = self._config_model(**step_config_dict)\n",
    "\n",
    "    @property\n",
    "    def processor(self) -> FrameworkProcessor:\n",
    "        # Start with init args from config, but convert TypedDict to dict so we can modify keys.\n",
    "        init_args: dict[str, Any] = dict(self._config.processor_init_args)\n",
    "        # Replace the string of estimator_cls_name with the actual estimator_cls\n",
    "        estimator_cls_name = init_args.pop('estimator_cls_name')\n",
    "        init_args['estimator_cls'] = self.estimator_name_to_cls_mapping[estimator_cls_name]\n",
    "        return FrameworkProcessor(**init_args)  # todo: check if typechecker catches wrong args. Otherwise, define typed dict for FWPInitArgs.\n",
    "\n",
    "    def get_run_args(self) -> dict[str, Any]:\n",
    "        # Start with init args from config, but convert TypedDict to dict so we can modify keys.\n",
    "        run_args: dict[str, Any] = dict(self._config.processor_run_args)\n",
    "        # Create ProcessingInputs from list of s3paths (strings)\n",
    "        _input_files_s3paths: list[str] = run_args.pop('input_files_s3paths')\n",
    "        _processing_inputs: list[ProcessingInput] = [\n",
    "            ProcessingInput(\n",
    "                source=s3path,\n",
    "                # todo: Allow passing through extra arguments\n",
    "            )\n",
    "            for s3path in _input_files_s3paths\n",
    "        ]\n",
    "        run_args['inputs'] = _processing_inputs\n",
    "        return run_args\n",
    "\n",
    "    def create_step(self) -> ProcessingStep:\n",
    "        return ProcessingStep(\n",
    "            processor=self.processor,\n",
    "            **self.get_run_args()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepFactory *Facade*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "StepFactoryLookupTableType: TypeAlias = dict[str, type[StepFactoryInterface]]\n",
    "\n",
    "default_stepfactory_lookup_table: StepFactoryLookupTableType = {\n",
    "    'FrameworkProcessor': FrameworkProcessingStepFactory,\n",
    "}\n",
    "\n",
    "class StepFactoryFacade:\n",
    "    \"\"\"\n",
    "    Relationship between façade and concrete factories: A pipeline will generally have a *single* instance of  this façade, which in turn will create an instance of a concrete factory for every step it needs to create.\n",
    "\n",
    "    This class serves as a façade for creating steps that abstracts the following tasks from the user:\n",
    "    - It receives the step name from the user, based on which it retrieves the associated config for that step.\n",
    "    - From that config, it looks up what kind of step the user wants to create.\n",
    "    - It looks up which factory it should use for creating that kind of step. To be able to do so, it has a lookup table that maps step names to factory classes. (Note that this lookup table needs to be provided during instantiation. However, this library will also expose an instance of the StepFactoryFaçade that has already been initialized with a default lookup table, which will make the library even easier to use for less advanced users).\n",
    "    - Great an instance of that specific step factory.\n",
    "    - Finally, it will delegate the creation of the actual step to that specific factory, and then return the resulting step to the user.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_config_dicts: list[dict[str, Any]],\n",
    "        stepfactory_lookup_table: StepFactoryLookupTableType = default_stepfactory_lookup_table\n",
    "    ):\n",
    "        self.step_config_dicts = step_config_dicts\n",
    "        self.stepfactory_lookup_table = stepfactory_lookup_table\n",
    "\n",
    "    def _create_individual_step(\n",
    "        self,\n",
    "        step_config_dict: dict[str, Any]\n",
    "    ) -> ConfigurableRetryStep:\n",
    "\n",
    "        # Get the right *class* of step factory for a given step (based on its config)\n",
    "        factory_cls_name: str = step_config_dict['step_factory_class']\n",
    "        StepFactory_cls: type[StepFactoryInterface] = self.stepfactory_lookup_table[factory_cls_name]\n",
    "\n",
    "        # Instantiate factory, using step config. Then create step\n",
    "        step_factory = StepFactory_cls(step_config_dict=step_config_dict)\n",
    "        return step_factory.create_step()\n",
    "\n",
    "    def create_all_steps(self) -> list[ConfigurableRetryStep]:\n",
    "        steps: list[ConfigurableRetryStep] = []\n",
    "        for config in self.step_config_dicts:\n",
    "            step: ConfigurableRetryStep = self._create_individual_step(config)\n",
    "            steps.append(step)\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Note that the StepFactoryWrapper is decoupled from the specific StepFactory that will be used to create the step. The latter is determined by a lookup table, which is injected into to the StepFactoryWrapper during instantiation.~~\n",
    "\n",
    "~~The downside is that this is less convenient for simple use cases, where the user is content with choosing only from the default factories that ship with the library. To remediate this disadvantage, we can simply create a facade, which instantiates the StepFactoryWrapper with the default lookup table. More advanced users, by contrast, can directly import this default lookup table and customize it to point to custom StepFactory implementations. In a second step, they then initialize the StepFactoryWrapper directly, passing it the custom lookup table.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>18: \u001b[1m\u001b[31merror:\u001b[m Missing return statement  \u001b[m\u001b[33m[empty-body]\u001b[m\n",
      "<cell>18: \u001b[34mnote:\u001b[m If the method is meant to be abstract, use @abc.abstractmethod\u001b[m\n",
      "<cell>31: \u001b[1m\u001b[31merror:\u001b[m Incompatible types in assignment (expression has type \u001b[m\u001b[1m\"ellipsis\"\u001b[m, variable has type \u001b[m\u001b[1m\"list[Path]\"\u001b[m)  \u001b[m\u001b[33m[assignment]\u001b[m\n"
     ]
    }
   ],
   "source": [
    "class ConfigLoaderInterface(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def shared_config(self) -> SharedConfigInterface:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def step_configs_as_dicts(self) -> list[dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "\n",
    "class ConfigLoader(ConfigLoaderInterface):\n",
    "    def __init__(self, config_folder: Path, shared_config_model: type[SharedConfigInterface]):\n",
    "        self._config_folder = config_folder\n",
    "        self._shared_config_model = shared_config_model\n",
    "\n",
    "    def _load_config(self, config_file: Path) -> dict[str, Any]:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def shared_config(self) -> SharedConfigInterface:\n",
    "        path_to_shared_config: Path = self._config_folder / 'shared_config.yaml'\n",
    "        return self._shared_config_model(\n",
    "            **self._load_config(config_file=path_to_shared_config),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def step_configs_as_dicts(self) -> list[dict[str, Any]]:\n",
    "        # Traverses the config directory and returns names of all subfolders, each of which will correspond to a step name.\n",
    "        step_config_paths: list[Path] = ...\n",
    "        return [\n",
    "            self._load_config(config_path)\n",
    "            for config_path in step_config_paths\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Pipeline* Facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "from sm_pipelines_oo.aws_connector.implementation import create_aws_connector\n",
    "from sm_pipelines_oo.shared_config_schema import Environment\n",
    "\n",
    "\n",
    "class PipelineFacade:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_folder: Path,\n",
    "        environment: Environment,\n",
    "        shared_config_model: type[SharedConfigInterface], # todo: decide how best to get this\n",
    "    ):\n",
    "\n",
    "        # Derived attributes\n",
    "        self._config_loader = ConfigLoader(\n",
    "            config_folder=config_folder,\n",
    "            shared_config_model=shared_config_model,\n",
    "        )\n",
    "        self.shared_config: SharedConfigInterface = self._config_loader.shared_config\n",
    "        self.step_factory_facade = StepFactoryFacade(\n",
    "            step_config_dicts=self._config_loader.step_configs_as_dicts, # todo: pass in method call again?\n",
    "        )\n",
    "\n",
    "        self.aws_connector = create_aws_connector(\n",
    "            run_as_pipeline=True,\n",
    "            shared_config=self.shared_config,\n",
    "            environment=environment,\n",
    "        )\n",
    "\n",
    "\n",
    "    @property\n",
    "    def pipeline(self):\n",
    "        return Pipeline(\n",
    "            name=self.shared_config.project_name,\n",
    "            # parameters=[],\n",
    "            steps=self.create_steps(),\n",
    "            sagemaker_session=...\n",
    "        )\n",
    "\n",
    "    def create_steps(self):\n",
    "        return self.step_factory_facade.create_all_steps()\n",
    "\n",
    "    def run(self) -> None:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of library code\n",
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _config = FrameworkProcessingStepConfig(\n",
    "#     step_name=\"preprocessing\",\n",
    "#     processor_init_args={\n",
    "#         \"framework_version\": \"0.0.1\",\n",
    "#         \"estimator_cls\": \"estimator\",\n",
    "#         \"instance_count\": 1,\n",
    "#         \"instance_type\": \"ml.m5.large\",\n",
    "#     },\n",
    "#     processor_run_args={\n",
    "#         \"code\": \"code\",\n",
    "#         \"source_dir\": \"source_dir\",\n",
    "#         \"input_files\": [\"input_files\"],\n",
    "#         \"output_files\": [\"output_files\"],\n",
    "#     },\n",
    "#     shared_config=SharedConfigInterface(\n",
    "#         project_name=\"project_name\",\n",
    "#         project_version=\"project_version\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_facade = PipelineFacade(stepfactory_lookup_table=stepfactory_lookup_table)\n",
    "# pipeline_facade.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm-pipelines-oo-tWfBw0_D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
