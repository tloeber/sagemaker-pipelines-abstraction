{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Leverage **Factory Method Pattern**: \n",
    "- StepFactory will be an abstract class with two abstract methods for instantiating the step actor (processor, etc.), and constructing the step's run args.\n",
    "- StepFactory provides an implementation for step. This is achieved by calling the two abstract methods internally.\n",
    "- User has to implement these abstract methods (how exactly to instantiate the step's actor, and how to construct its run args).\n",
    "\n",
    "Why I **discarded this design**:\n",
    "- While this design  makes it easier on the user to define different kinds of ProcessingSteps, because   it breaks it down into two easier problems, this design does not generalize to other steps. For example, rather than calling a  \"run\" method, training and human steps instead require a calling \"fit\" method. While it would  probably be possible to work around  lists, a ConditionStep  does not follow this pattern at all.\n",
    "\n",
    " **Lesson: Don't  construct step using `ProcessingStep(step_args=actor.run(inputs=..., ...))`.  Rather, pass an instance of the actor  and individual run args separately, i.e. `ProcessingStep(processor=my_processor, inputs=..., ...)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_mypy extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_mypy\n"
     ]
    }
   ],
   "source": [
    "%load_ext nb_mypy\n",
    "\n",
    "from typing import Generic, TypeVar, TypeAlias, Any, final\n",
    "from abc import abstractmethod\n",
    "\n",
    "from sagemaker.processing import Processor, FrameworkProcessor\n",
    "from sagemaker.estimator import EstimatorBase\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep, \\\n",
    "    TuningStep, ConfigurableRetryStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sagemaker_submit_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# instantiate factory and create step\u001b[39;00m\n\u001b[1;32m     40\u001b[0m fwp_factory \u001b[38;5;241m=\u001b[39m FrameworkProcessingStepFactory()\n\u001b[0;32m---> 41\u001b[0m \u001b[43mfwp_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m, in \u001b[0;36mStepFactory.create_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m step_actor: StepActor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instantiate_step_actor()\n\u001b[1;32m     24\u001b[0m run_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_run_args()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sm-pipelines-oo-tWfBw0_D-py3.10/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sm-pipelines-oo-tWfBw0_D-py3.10/lib/python3.10/site-packages/sagemaker/processing.py:1766\u001b[0m, in \u001b[0;36mFrameworkProcessor.run\u001b[0;34m(self, code, source_dir, dependencies, git_config, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;129m@runnable_by_pipeline\u001b[39m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     kms_key: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1662\u001b[0m ):\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a processing job.\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m        :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1766\u001b[0m     s3_runproc_sh, inputs, job_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pack_and_upload_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkms_key\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# Submit a processing job.\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m   1772\u001b[0m         code\u001b[38;5;241m=\u001b[39ms3_runproc_sh,\n\u001b[1;32m   1773\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         kms_key\u001b[38;5;241m=\u001b[39mkms_key,\n\u001b[1;32m   1781\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sm-pipelines-oo-tWfBw0_D-py3.10/lib/python3.10/site-packages/sagemaker/processing.py:1793\u001b[0m, in \u001b[0;36mFrameworkProcessor._pack_and_upload_code\u001b[0;34m(self, code, source_dir, dependencies, git_config, job_name, inputs, kms_key)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1791\u001b[0m     job_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_current_job_name(job_name)\n\u001b[0;32m-> 1793\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_payload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgit_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_inputs_with_payload(\n\u001b[1;32m   1801\u001b[0m     inputs,\n\u001b[1;32m   1802\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker_submit_directory\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1803\u001b[0m )\n\u001b[1;32m   1805\u001b[0m local_code \u001b[38;5;241m=\u001b[39m get_config_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal.local_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sm-pipelines-oo-tWfBw0_D-py3.10/lib/python3.10/site-packages/sagemaker/processing.py:1888\u001b[0m, in \u001b[0;36mFrameworkProcessor._upload_payload\u001b[0;34m(self, entry_point, source_dir, dependencies, git_config, job_name)\u001b[0m\n\u001b[1;32m   1877\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_estimator(\n\u001b[1;32m   1878\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39mentry_point,\n\u001b[1;32m   1879\u001b[0m     source_dir\u001b[38;5;241m=\u001b[39msource_dir,\n\u001b[1;32m   1880\u001b[0m     dependencies\u001b[38;5;241m=\u001b[39mdependencies,\n\u001b[1;32m   1881\u001b[0m     git_config\u001b[38;5;241m=\u001b[39mgit_config,\n\u001b[1;32m   1882\u001b[0m )\n\u001b[1;32m   1884\u001b[0m estimator\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1885\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1887\u001b[0m     estimator\u001b[38;5;241m.\u001b[39msource_dir,\n\u001b[0;32m-> 1888\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msagemaker_submit_directory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sagemaker_submit_directory'"
     ]
    }
   ],
   "source": [
    "# todo: decide how stringent this should be while still allowing user to add new step types\n",
    "StepType = TypeVar(\"StepType\", bound=ConfigurableRetryStep)\n",
    "StepActor: TypeAlias = Processor |  EstimatorBase  # todo: add more types as needed\n",
    "\n",
    "\n",
    "class BaseStepFactory(Generic[StepType]):\n",
    "    # @abstractmethod\n",
    "    # def __init__(self, step: StepType):\n",
    "    #     self.step_cls = StepType\n",
    "\n",
    "    @abstractmethod\n",
    "    def instantiate_step_actor(self) -> StepActor:\n",
    "        \"\"\"\n",
    "        This method  is used internally by the factory method. However, it can also be used to instantiate a step actor (e.g., processor) directly for a quicker iteration during development.\n",
    "\n",
    "        Note: It is consistent with LSP for the implementation to return a more *specific* type.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def _construct_run_args(self) -> dict[str, Any]:  # todo: create dataclass for return types\n",
    "        \"\"\"\n",
    "        Note: It is consistent with LSP for the implementation to return a more *specific* type.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @final\n",
    "    def create_step(self) -> StepType:\n",
    "        \"\"\"\n",
    "        This is the factory method. It is not meant to be overridden. Instead, subclasses should implement the two abstract methods, which in turn specify what exactly this factory method will do.\n",
    "        \"\"\"\n",
    "        # Instantiate the actor (e.g., processor) for the step\n",
    "        step_actor: StepActor = self.instantiate_step_actor()\n",
    "        run_args: dict[str, Any] = self._construct_run_args()\n",
    "        return step_actor.run(**run_args)\n",
    "\n",
    "\n",
    "# Create concrete factory class by implementing abstract methods\n",
    "class FrameworkProcessingStepFactory(BaseStepFactory[ProcessingStep]):\n",
    "    def instantiate_step_actor(self) -> FrameworkProcessor:  # Note the more specific return type\n",
    "        return FrameworkProcessor(\n",
    "            estimator_cls=SKLearn, framework_version='0.23-1', role='role', instance_type=''\n",
    "        )\n",
    "\n",
    "    def _construct_run_args(self) -> dict[str, Any]:\n",
    "        return {'inputs': [], 'outputs': [], 'source_dir': '', 'code': ''}\n",
    "\n",
    "\n",
    "# instantiate factory and create step\n",
    "fwp_factory = FrameworkProcessingStepFactory()\n",
    "fwp_factory.create_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: while we get an error here, this is due to the fact that we have  not provided all the necessary configuration to instantiate the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not abstract factory pattern?\n",
    "On the surface it may seem like the abstract factory pattern would be a natural fit, as it creates **families of *related* products**. In our case, by contrast, **we want the user to be able to pick and choose from any step factory fits their use case.** This is better served by having factories that create only a single type of step. There is no reason to force the user to pick a single overarching factory that constrains the steps to a family of related implementations. All we care about is that all the produced steps satisfies the common StepInterface.\n",
    "\n",
    "For example, the user may want to use two different types of ProcessingSteps  in the same pipeline, one based on a FrameworkProcessor and one on a different kind of Processor. To allow this, our abstract factory would have to  defined interfaces for each specific step type, which is at a too low level of abstraction, and would require a change to the interface  every time we want to support a new kind of processor.  Since interfaces are supposed to be stable, this is a huge downside."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm-pipelines-oo-tWfBw0_D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
