{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 1.0.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make imports and folder paths work\n",
    "# todo: Instead create python package and install locally\n",
    "import os, sys\n",
    "os.chdir(\n",
    "    f'{os.environ[\"HOME\"]}/repos/sagemaker-pipelines-abstraction/src'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from functools import cached_property\n",
    "from typing import Literal, Callable, TypeAlias\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import TypeVar, Generic\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "from loguru import logger\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import Step\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.workflow.steps import ConfigurableRetryStep, ProcessingStep\n",
    "from sm_pipelines_oo.shared_config_schema import Environment\n",
    "\n",
    "from sm_pipelines_oo.shared_config_schema import SharedConfig, Environment\n",
    "# from sm_pipelines_oo.steps.interfaces import StepFactoryInterface\n",
    "from sm_pipelines_oo.connector.interface import AWSConnectorInterface\n",
    "from sm_pipelines_oo.utils import load_pydantic_config_from_file\n",
    "from sm_pipelines_oo.connector.interface import AWSConnectorInterface\n",
    "from sm_pipelines_oo.connector.implementation import create_aws_connector\n",
    "from sm_pipelines_oo.pipeline_wrapper import PipelineWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedConfig(BaseSettings):  # type: ignore\n",
    "    \"\"\"Defines configuration shared by all pipeline steps (for a given environment).\"\"\"\n",
    "    project_name: str\n",
    "    project_version: str  # Versions data (and probably more in the future)\n",
    "    region: str\n",
    "    # To do: consider which of these fields should be made required.\n",
    "    role_name: str | None = None\n",
    "    project_bucket_name: str\n",
    "\n",
    "shared_config: SharedConfig = SharedConfig(\n",
    "    project_name='design-decisions',\n",
    "    project_version='0.0',  # Versions data (and probably more in the future)\n",
    "    region='local',\n",
    "    # To do: consider which of these fields should be made required.\n",
    "    role_name=None,\n",
    "    project_bucket_name='design-decisions',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingConfig(BaseSettings):  # type: ignore\n",
    "    input_filename: str\n",
    "    instance_type: str\n",
    "    instance_count: int\n",
    "    sklearn_framework_version: str\n",
    "    # Don't set in config. This needs to correspond to SM's convention for local folder structure.\n",
    "    # todo: Make this not set-able. Use property instead?\n",
    "    step_type: Literal['processing'] = \"processing\"\n",
    "    step_name: str = \"processing\"\n",
    "\n",
    "processing_config = ProcessingConfig(\n",
    "    input_filename='input.parquet',\n",
    "    instance_type='local',\n",
    "    instance_count=1,\n",
    "    sklearn_framework_version='0.23-1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current design: Shared config *not* accessible to step factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>1: \u001b[1m\u001b[31merror:\u001b[m Name \u001b[m\u001b[1m\"PipelineWrapper\"\u001b[m already defined (possibly by an import)  \u001b[m\u001b[33m[no-redef]\u001b[m\n",
      "<cell>18: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"shared_config\"\u001b[m for \u001b[m\u001b[1m\"create_step\"\u001b[m of \u001b[m\u001b[1m\"StepFactoryInterface\"\u001b[m  \u001b[m\u001b[33m[call-arg]\u001b[m\n"
     ]
    }
   ],
   "source": [
    "class PipelineWrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_factories: list[StepFactoryInterface],\n",
    "        environment: Environment,\n",
    "        shared_config: SharedConfig,\n",
    "        aws_connector: AWSConnectorInterface,\n",
    "    ) -> None:\n",
    "        self.environment = environment\n",
    "        self.shared_config = shared_config\n",
    "        self._aws_connector = aws_connector\n",
    "\n",
    "        self.steps: list[Step] = []\n",
    "        self._create_steps(step_factories, shared_config)\n",
    "\n",
    "    def _create_steps(self, step_factories: list[StepFactoryInterface], shared_config: SharedConfig) -> None:\n",
    "        for factory in step_factories:\n",
    "            step: Step = factory.create_step(\n",
    "                shared_config=shared_config,\n",
    "            )\n",
    "            self.steps.append(step)\n",
    "\n",
    "    @cached_property\n",
    "    def _pipeline(self) -> Pipeline:\n",
    "        pipeline_name = f'{self.shared_config.project_name}-{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "        pipeline = Pipeline(\n",
    "            name=pipeline_name,\n",
    "            steps=self.steps,\n",
    "            sagemaker_session=self._aws_connector.sm_session,\n",
    "        )\n",
    "        pipeline.create(role_arn=self._aws_connector.role_arn)\n",
    "        return pipeline\n",
    "\n",
    "\n",
    "    # Public methods\n",
    "    # ==============\n",
    "\n",
    "    def run(self) -> None:\n",
    "        logger.info(f\"Starting pipeline run for project {self.shared_config.project_name}\")\n",
    "        execution = self._pipeline.start()\n",
    "        execution.wait()\n",
    "        execution.list_steps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>4: \u001b[1m\u001b[31merror:\u001b[m Name \u001b[m\u001b[1m\"ENVIRONMENT\"\u001b[m is not defined  \u001b[m\u001b[33m[name-defined]\u001b[m\n",
      "<cell>9: \u001b[1m\u001b[31merror:\u001b[m Name \u001b[m\u001b[1m\"ProcessingStepFactory\"\u001b[m is not defined  \u001b[m\u001b[33m[name-defined]\u001b[m\n",
      "<cell>12: \u001b[1m\u001b[31merror:\u001b[m Name \u001b[m\u001b[1m\"config_path_pre_processing\"\u001b[m is not defined  \u001b[m\u001b[33m[name-defined]\u001b[m\n",
      "<cell>19: \u001b[1m\u001b[31merror:\u001b[m Name \u001b[m\u001b[1m\"ENVIRONMENT\"\u001b[m is not defined  \u001b[m\u001b[33m[name-defined]\u001b[m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ENVIRONMENT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ===\u001b[39;00m\n\u001b[1;32m      3\u001b[0m aws_connector: AWSConnectorInterface \u001b[38;5;241m=\u001b[39m create_aws_connector(\n\u001b[0;32m----> 4\u001b[0m     environment\u001b[38;5;241m=\u001b[39m\u001b[43mENVIRONMENT\u001b[49m,\n\u001b[1;32m      5\u001b[0m     shared_config\u001b[38;5;241m=\u001b[39mshared_config,\n\u001b[1;32m      6\u001b[0m     run_as_pipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m pre_processing_step_factory \u001b[38;5;241m=\u001b[39m ProcessingStepFactory(\n\u001b[1;32m     10\u001b[0m     processor_cls\u001b[38;5;241m=\u001b[39mFrameworkProcessor,\n\u001b[1;32m     11\u001b[0m     processor_extra_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator_cls\u001b[39m\u001b[38;5;124m'\u001b[39m: SKLearn},\n\u001b[1;32m     12\u001b[0m     step_config_path\u001b[38;5;241m=\u001b[39mconfig_path_pre_processing,\n\u001b[1;32m     13\u001b[0m     aws_connector\u001b[38;5;241m=\u001b[39maws_connector,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m PipelineWrapper(\n\u001b[1;32m     16\u001b[0m     step_factories\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     17\u001b[0m         pre_processing_step_factory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     aws_connector\u001b[38;5;241m=\u001b[39maws_connector,\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ENVIRONMENT' is not defined"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "# ===\n",
    "aws_connector: AWSConnectorInterface = create_aws_connector(\n",
    "    environment=ENVIRONMENT,\n",
    "    shared_config=shared_config,\n",
    "    run_as_pipeline=True,\n",
    ")\n",
    "\n",
    "pre_processing_step_factory = ProcessingStepFactory(\n",
    "    processor_cls=FrameworkProcessor,\n",
    "    processor_extra_kwargs={'estimator_cls': SKLearn},\n",
    "    step_config_path=config_path_pre_processing,\n",
    "    aws_connector=aws_connector,\n",
    ")\n",
    "pipeline = PipelineWrapper(\n",
    "    step_factories=[\n",
    "        pre_processing_step_factory,\n",
    "    ],\n",
    "    environment=ENVIRONMENT,\n",
    "    shared_config=shared_config,\n",
    "    aws_connector=aws_connector,\n",
    ")\n",
    "try:\n",
    "    pipeline.run()\n",
    "except Exception as e:\n",
    "    logger.error(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative: Shared config *is* accessible to step factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sagemaker.workflow.steps import ConfigurableRetryStep, ProcessingStep\n",
    "\n",
    "# *General* step factory interface\n",
    "# ==============================\n",
    "# class StepFactoryInterface(ABC):\n",
    "\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def create_step(self) -> ConfigurableRetryStep:\n",
    "#         ...\n",
    "\n",
    "#     @property\n",
    "#     @abstractmethod\n",
    "#     def _step_name(self) -> str:\n",
    "#         ...\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def _get_step_config(self, environment: Environment) -> dict:  # todo: improve type\n",
    "#         ...\n",
    "\n",
    "\n",
    "class ProcessingStepFactoryInterface(StepFactoryInterface):\n",
    "    # todo: add specific methods\n",
    "    ...\n",
    "class FrameworkProcessorFactory(ProcessingStepFactoryInterface):\n",
    "    def __init__(\n",
    "            self,\n",
    "            step_name: str,  # identifies step config (given env)\n",
    "        ) -> None:\n",
    "            self._step_name = step_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sm_pipelines_oo.connector.interface import AWSConnectorInterface\n",
    "from sm_pipelines_oo.connector.implementation import AWSConnector, LocalAWSConnector\n",
    "\n",
    "class PipelineWrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_factories: list[StepFactoryInterface],\n",
    "        environment: Environment,\n",
    "        shared_config: SharedConfig,\n",
    "        aws_connector: AWSConnectorInterface,\n",
    "    ) -> None:\n",
    "        self.environment: Environment = environment\n",
    "        self.shared_config = shared_config\n",
    "        self._aws_connector = aws_connector\n",
    "\n",
    "        self.steps: list[Step] = []\n",
    "        self._create_steps(step_factories, shared_config)\n",
    "\n",
    "    @cached_property\n",
    "    def _aws_connector(self) -> AWSConnectorInterface:\n",
    "        \"\"\"\n",
    "        This code makes connector.implementation.create_aws_connector() redundant, except for use\n",
    "        outside of pipeline.\n",
    "        Todo: decide where to put code for the latter case.\n",
    "        \"\"\"\n",
    "        if self.environment == 'local':\n",
    "            return LocalAWSConnector()\n",
    "        else:\n",
    "            return AWSConnector(\n",
    "                environment=self.environment,\n",
    "                # this error will resolve once we don't use SharedConfig from this notebook but\n",
    "                # library's AWSConnector.\n",
    "                shared_config=self.shared_config,  # type: ignore\n",
    "                run_as_pipeline=True\n",
    "            )\n",
    "\n",
    "\n",
    "    def _create_steps(self, step_factories: list[StepFactoryInterface], shared_config: SharedConfig) -> None:\n",
    "        for factory in step_factories:\n",
    "            step: Step = factory.create_step()\n",
    "            self.steps.append(step)\n",
    "\n",
    "    @cached_property\n",
    "    def _pipeline(self) -> Pipeline:\n",
    "        pipeline_name = f'{self.shared_config.project_name}-{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "        pipeline = Pipeline(\n",
    "            name=pipeline_name,\n",
    "            steps=self.steps,\n",
    "            sagemaker_session=self._aws_connector.sm_session,\n",
    "        )\n",
    "        pipeline.create(role_arn=self._aws_connector.role_arn)\n",
    "        return pipeline\n",
    "\n",
    "\n",
    "    # Public methods\n",
    "    # ==============\n",
    "\n",
    "    def run(self) -> None:\n",
    "        logger.info(f\"Starting pipeline run for project {self.shared_config.project_name}\")\n",
    "        execution = self._pipeline.start()\n",
    "        execution.wait()\n",
    "        execution.list_steps()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm-pipelines-oo-tWfBw0_D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
