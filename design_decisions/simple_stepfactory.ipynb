{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old explanation of design (overhaul/integrate):\n",
    "\n",
    "By contrast to earlier designs, create_step does not require a step_config to be passed here as an argument. This simplify the interface considerably because we don't have to worry about getting the right type of step_config for a given step_type. The problem is that while we can simply *return more specific* step types in subclasses (such as a ProcessingStep instead of a ConfigurableRetryStep), we can**not** require a *more specific argument* for subclasses (for example requiring a ProcessingStepConfig for a ProcessingStepFactory), as this would violate the Liskov Substitution Principal.\n",
    "\n",
    "This problem would not even be easily solved by using generics, because it is not obvious how we can go from my given Steptype to the associated StepConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make local *folder* paths work (even though python paths work due to packageing)\n",
    "import os\n",
    "os.chdir(\n",
    "    f'{os.environ[\"HOME\"]}/repos/sagemaker-pipelines-abstraction/src/sm_pipelines_oo/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import  ClassVar, TypeVar, TypeAlias, Any, final, TypedDict\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "from sagemaker.processing import Processor\n",
    "from sagemaker.estimator import EstimatorBase\n",
    "\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep, \\\n",
    "    TuningStep, ConfigurableRetryStep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why no generics?\n",
    "- The end goal is simply to have an object that satisfies the (ConfigurableRetry)StepInterface. From the perspective of the pipeline, we don't care what type of step it is.\n",
    "- The initial reason for looking into leveraging generics was for making sure that we are passing the right config for a given type of step. However, after a lot of trial and error, I still did not find a good way to create a simple class hierarchy based on what the Sagemaker SDK makes available to us. Instead, it looks more promising to simply create a very minimal interface for step factories, and later specific implementations decide what the best way to create that kind of step is.\n",
    "  - Downside: let's reuse of code between different step factories. This makes it somewhat harder to get started with creating new step factories, because there is less structure imposed for how exactly to do it.\n",
    "  - Upside: more flexibility for grading step factories. This may actually make it easier to create new step factories, and it will make it easier to maintain given step factories as the interface of the Sagemaker SDK changes.\n",
    "  - Note: neither of these points will affect a basic library user who only uses inbuilt step factories.\n",
    "\n",
    "# Configuration\n",
    "Goal:  Abstract configuration into a single config class which loads all config's it needs in the directory (even if this requires traversing). This will not only make the intent of this method more clear, but it will also make it easier to have a single config façade that abstracts what config's are global and what are step-specific (step config simply need a reference to the shared config, so they can fall back to that if necessary, but the concrete logic can be implemented differently for each step type). Also, having a config façade makes it easy to define methods that compute derived values.\n",
    "\n",
    "## Attempt 1: *Overarching* ConfigFacade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConfigFacade:\n",
    "#     def __init__(self, config_dir: Path):\n",
    "#         # load all the yaml config files\n",
    "#         shared_config_dict: dict[str, Any] = ...\n",
    "#         step_configs_dicts: dict[str, dict[str, Any]] = ...\n",
    "\n",
    "#         # Convert the dictionaries to pydantic models\n",
    "#         self.shared_config: SharedConfig = SharedConfig(**shared_config_dict)\n",
    "#         self.step_configs: dict[str, BaseSettings] = {}\n",
    "#         for step_name, step_config_dict  in step_configs_dicts.items():\n",
    "\n",
    "#             self.step_configs[step_name] = StepConfig(**step_config_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: how to get  the pydantic model for a given config? While it would be possible to have another look up table, similar to how we find the right specific step factory, it makes more sense that each step factory owns the associated config model. This is because the key challenge is that the  config model matches the specific factory. \n",
    "\n",
    "As a result, it  is better to not load all the configs upfront (except possibly into dictionaries).\n",
    "\n",
    "## Attempt 2: *Separate* Configs w/o facade, but *reference* to shared config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "\n",
    "# class SharedConfigInterface(BaseSettings):\n",
    "#     \"\"\"\n",
    "#     This interface defines all the configs that our library code expects to be present in the shared_config.\n",
    "#     \"\"\"\n",
    "#     project_name: str\n",
    "#     project_version: str  # Versions data (and probably more in the future)\n",
    "\n",
    "\n",
    "# class StepConfigInterface(BaseSettings):\n",
    "#     \"\"\"\n",
    "#     This ensures every step config has a step_type (required to determine step factory),  as well as a reference to the shared_config.\n",
    "\n",
    "#     Note: If the concrete step_config depends on any specific config values being set in the shared_config (in addition to the ones defined in the SharedConfigInterface), we should redefine the type of shared_config to this more specific type.\n",
    "#     \"\"\"\n",
    "#     step_type: str # Identifies factory, which in turn identifies StepConfig model\n",
    "#     shared_config: SharedConfigInterface # So that we have access to sharedconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple factory\n",
    "~~This makes better use of factory, because depending on the arg passed to it, it creates a different type of step. Otherwise, we may as well us strategy pattern (only use of factory is to construct step later when configs etc are known - but a given factory always produces same kind of step, except from configuration).~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session, get_execution_role\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "\n",
    "class StepFactoryInterface(ABC):\n",
    "    \"\"\"\n",
    "    In addition to the required methods defined below, it is recommended to implement the following attributes and methods in order to make implementation of the required methods easiest:\n",
    "    - _config_model: ClassVar[type[BaseSettings]] (Class used to convert config_dict to pydantic model to validate types and potentially compute derived attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_config_dict: dict[str, Any],\n",
    "        role_arn: str,\n",
    "        pipeline_session: PipelineSession | LocalPipelineSession, # todo: consider allowing normal session - probably should be separate argument though?\n",
    "    ):\n",
    "        ...\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # @abstractmethod\n",
    "    # def _get_config_model() -> type[BaseSettings]:\n",
    "    #     \"\"\"\n",
    "    #     Pydantic model used to validate and convert the config_dict to an instance of pydantic.BaseSettings.\n",
    "    #     \"\"\"\n",
    "    #     ...\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_step(self) -> ConfigurableRetryStep:\n",
    "        # Note that we don't have to worry about violating the LSP -  even though we are adding back an argument for the config – because at this stage that config will simply be of type dictionary. Thus, subclasses don't have to specify a more specific subtype of config here yet.\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python < 3.12, don't use typing.TypedDict: https://docs.pydantic.dev/2.6/errors/usage_errors/#typed-dict-version\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.estimator import EstimatorBase\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "from sm_pipelines_oo.shared_config_schema import SharedConfig\n",
    "\n",
    "class _FWProcessorInitConfig(TypedDict):\n",
    "    framework_version: str\n",
    "    estimator_cls_name: str\n",
    "    instance_count: int\n",
    "    instance_type: str\n",
    "\n",
    "\n",
    "class _FWProcessorRunConfig(TypedDict):\n",
    "    code: str\n",
    "    source_dir: str\n",
    "    # todo: allow athena datasetdefinition instead\n",
    "    input_files_s3paths: list[str]  # todo: validate it's an s3 path\n",
    "    output_files_s3paths: list[str]  # todo: validate it's an s3 path\n",
    "\n",
    "\n",
    "class FrameworkProcessingStepConfig(BaseSettings):\n",
    "    # todo:\n",
    "    step_name: str\n",
    "    step_factory_class: str\n",
    "    processor_init_args: _FWProcessorInitConfig\n",
    "    processor_run_args: _FWProcessorRunConfig\n",
    "    # For now, we will reload this for every step config to avoid dependency on pipeline wrapper.\n",
    "    shared_config: SharedConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "\n",
    "class FrameworkProcessingStepFactory(StepFactoryInterface):\n",
    "    # Note: this is a public attribute, so user can add support for additional estimators\n",
    "    estimator_name_to_cls_mapping: ClassVar[dict[str, Any]] = {  # todo:  find supertype\n",
    "        'SKLearn': SKLearn,\n",
    "    }\n",
    "\n",
    "    _config_model: ClassVar[type[FrameworkProcessingStepConfig]] = FrameworkProcessingStepConfig\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_config_dict: dict[str, Any],\n",
    "        role_arn: str,\n",
    "        pipeline_session: PipelineSession | LocalPipelineSession\n",
    "    ):\n",
    "        # Parse config, using the specific pydantic model that this factory has as a class variable.\n",
    "        self._config: FrameworkProcessingStepConfig = self._config_model(**step_config_dict)\n",
    "        self._role_arn = role_arn\n",
    "        self._pipeline_session = pipeline_session\n",
    "\n",
    "    @property\n",
    "    def processor(self) -> FrameworkProcessor:\n",
    "        # Start with init args from config, but convert TypedDict to dict so we can modify keys.\n",
    "        init_args: dict[str, Any] = dict(self._config.processor_init_args)\n",
    "        # Replace the string of estimator_cls_name with the actual estimator_cls\n",
    "        estimator_cls_name = init_args.pop('estimator_cls_name')\n",
    "        init_args['estimator_cls'] = self.estimator_name_to_cls_mapping[estimator_cls_name]\n",
    "        return FrameworkProcessor(\n",
    "            **init_args,\n",
    "            role=self._role_arn,\n",
    "            sagemaker_session=self._pipeline_session,\n",
    "        )  # todo: check if typechecker catches wrong args. Otherwise, define typed dict for FWPInitArgs.\n",
    "\n",
    "    def _construct_run_args(self) -> dict[str, Any]:\n",
    "        # Start with init args from config, but convert TypedDict to dict so we can modify keys.\n",
    "        run_args: dict[str, Any] = dict(self._config.processor_run_args)\n",
    "\n",
    "        # Create ProcessingInputs from list of s3paths (strings)\n",
    "        _input_files_s3paths: list[str] = run_args.pop('input_files_s3paths')\n",
    "        _processing_inputs: list[ProcessingInput] = [\n",
    "            ProcessingInput(\n",
    "                source=s3path,\n",
    "                # todo: Allow passing through extra arguments\n",
    "            )\n",
    "            for s3path in _input_files_s3paths\n",
    "        ]\n",
    "        run_args['inputs'] = _processing_inputs\n",
    "\n",
    "        # Do the same for ProcessingOutputs\n",
    "        _output_files_s3paths: list[str] = run_args.pop('output_files_s3paths')\n",
    "        _processing_outputs: list[ProcessingOutput] = [\n",
    "            ProcessingOutput(\n",
    "                source=s3path,\n",
    "                # todo: Allow passing through extra arguments\n",
    "            )\n",
    "            for s3path in _output_files_s3paths\n",
    "        ]\n",
    "\n",
    "        run_args['outputs'] = _processing_outputs\n",
    "        return run_args\n",
    "\n",
    "    def create_step(self) -> ProcessingStep:\n",
    "        _step_args = self.processor.run(\n",
    "            **self._construct_run_args()\n",
    "        )\n",
    "        return ProcessingStep(\n",
    "            name=self._config.step_name,\n",
    "            step_args=_step_args,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.tokens:Loading cached SSO token for ml\n"
     ]
    }
   ],
   "source": [
    "# To do: move those into unit tests\n",
    "from sagemaker.session import Session, get_execution_role\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "\n",
    "_fw_processor_config_dict = {\n",
    "    'step_name': 'preprocessing',\n",
    "    'step_factory_class': 'FrameworkProcessingStepFactory',\n",
    "    'processor_init_args': {\n",
    "        'framework_version': '0.23-1',\n",
    "        'estimator_cls_name': 'SKLearn',\n",
    "        'instance_count': 1,\n",
    "        'instance_type': 'ml.m5.xlarge',\n",
    "    },\n",
    "    'processor_run_args': {\n",
    "        'code': 'preprocess.py',\n",
    "        'source_dir': 'code/preprocess',\n",
    "        'input_files_s3paths': [],\n",
    "        'output_files_s3paths': [],\n",
    "    },\n",
    "    'shared_config': {\n",
    "        'project_name': 'test',\n",
    "        'project_version': '0',\n",
    "        'region': 'local',\n",
    "        'project_bucket_name': 'test-bucket',\n",
    "    }\n",
    "}\n",
    "\n",
    "fw_processing_step_factory = FrameworkProcessingStepFactory(\n",
    "    step_config_dict=_fw_processor_config_dict,\n",
    "    pipeline_session=LocalPipelineSession(),\n",
    "    role_arn=get_execution_role(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fw_processixng_step_factory.create_step()\n",
    "# fwp = fw_processing_step_factory.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [a for a in dir(fwp) if not a.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'preprocess.py',\n",
       " 'source_dir': 'code/preprocess',\n",
       " 'inputs': [],\n",
       " 'outputs': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To do: add unit test  but provide inputs and outputs, and make sure that the run_args are constructed correctly.\n",
    "fw_processing_step_factory._construct_run_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessingStep(name='preprocessing', display_name=None, description=None, step_type=<StepTypeEnum.PROCESSING: 'Processing'>, depends_on=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_processing_step_factory.create_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepFactory *Facade*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepFactoryFacadeInterface(ABC):\n",
    "    \"\"\"\n",
    "    This interface decouples the pipeline façade from the specific step factory first use. The pipeline façade only cares about this one method.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def create_all_steps(self) -> list[ConfigurableRetryStep]:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stepfactory_lookup_table: dict[str, type[StepFactoryInterface]] = {\n",
    "    'FrameworkProcessor': FrameworkProcessingStepFactory,\n",
    "}\n",
    "\n",
    "class StepFactoryFacade(StepFactoryFacadeInterface):\n",
    "    \"\"\"\n",
    "    Relationship between façade and concrete factories: A pipeline will generally have a *single* instance of  this façade, which in turn will create an instance of a concrete factory for every step.\n",
    "\n",
    "    This class serves as a façade for creating steps that abstracts the following tasks from the user:\n",
    "    - It receives the configs for all steps as a list of dictionaries.\n",
    "    - For each step config, it:\n",
    "      - Looks up which factory it should use for creating that kind of step. To be able to do so, it has a lookup table that maps step names to factory classes. (This lookup table can be provided during instantiation of this class, but there is also a default lookup table for standard use cases.)\n",
    "      - Creates an instance of that specific step factory.\n",
    "      - Delegates the creation of the actual step to that specific factory.\n",
    "    - Finally, it will return the resulting list containing all steps.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_config_dicts: list[dict[str, Any]],\n",
    "        role_arn: str,\n",
    "        pipeline_session: PipelineSession | LocalPipelineSession,\n",
    "        # Generally, user does not set this, but it's useful for testing and custom use cases.\n",
    "        stepfactory_lookup_table: dict[str, type[StepFactoryInterface]] = \\\n",
    "            default_stepfactory_lookup_table\n",
    "    ):\n",
    "        self._step_config_dicts = step_config_dicts\n",
    "        self._role_arn = role_arn\n",
    "        self._pipeline_session = pipeline_session\n",
    "        self.stepfactory_lookup_table = stepfactory_lookup_table\n",
    "\n",
    "    def _create_individual_step(\n",
    "        self,\n",
    "        step_config_dict: dict[str, Any]\n",
    "    ) -> ConfigurableRetryStep:\n",
    "\n",
    "        # Get the right *class* of step factory for a given step (based on its config)\n",
    "        factory_cls_name: str = step_config_dict['step_factory_class']\n",
    "        StepFactory_cls: type[StepFactoryInterface] = self.stepfactory_lookup_table[factory_cls_name]\n",
    "\n",
    "        # Instantiate factory, using step config. Then create step\n",
    "        step_factory: StepFactoryInterface = StepFactory_cls(\n",
    "            step_config_dict=step_config_dict,\n",
    "            role_arn=self._role_arn,\n",
    "            pipeline_session=self._pipeline_session\n",
    "        )\n",
    "        return step_factory.create_step()\n",
    "\n",
    "    def create_all_steps(self) -> list[ConfigurableRetryStep]:\n",
    "        steps: list[ConfigurableRetryStep] = []\n",
    "        for config in self._step_config_dicts:\n",
    "            step: ConfigurableRetryStep = self._create_individual_step(config)\n",
    "            steps.append(step)\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Note that the StepFactoryWrapper is decoupled from the specific StepFactory that will be used to create the step. The latter is determined by a lookup table, which is injected into to the StepFactoryWrapper during instantiation.~~\n",
    "\n",
    "~~The downside is that this is less convenient for simple use cases, where the user is content with choosing only from the default factories that ship with the library. To remediate this disadvantage, we can simply create a facade, which instantiates the StepFactoryWrapper with the default lookup table. More advanced users, by contrast, can directly import this default lookup table and customize it to point to custom StepFactory implementations. In a second step, they then initialize the StepFactoryWrapper directly, passing it the custom lookup table.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import final\n",
    "from functools import cached_property\n",
    "\n",
    "import yaml\n",
    "from sm_pipelines_oo.shared_config_schema import Environment\n",
    "\n",
    "\n",
    "class AbstractConfigLoader():\n",
    "    \"\"\"\n",
    "    Abstract factory for loading configs as dictionaries.\n",
    "    Concrete implementations will  implement a method for how to load a given config file, as well as an attribute of which file types to load.\n",
    "    This abstract class provides implementation for how to load both the shared config as well as all the steps configs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Environment,\n",
    "        config_root_folder: str = 'config',  # relative path from project root\n",
    "    ):\n",
    "        self._env = env\n",
    "        self._config_folder = Path(config_root_folder) / env\n",
    "\n",
    "    @final\n",
    "    @cached_property\n",
    "    def shared_config_as_dict(self) -> dict[str, Any]:\n",
    "        shared_config_path: Path = self._config_folder / f'shared_config.{self._file_type_to_load}'\n",
    "        return self._load_config(shared_config_path)\n",
    "\n",
    "    @final\n",
    "    @cached_property\n",
    "    def step_configs_as_dicts(self) -> list[dict[str, Any]]:\n",
    "        # Traverses the config directory and returns names of all subfolders, each of which will correspond to a step name.\n",
    "        step_config_paths: list[Path] = [\n",
    "            path for path in self._config_folder.iterdir() if path.suffix == self._file_type_to_load\n",
    "        ]\n",
    "        return [\n",
    "            self._load_config(config_path)\n",
    "            for config_path in step_config_paths\n",
    "        ]\n",
    "\n",
    "    # Abstract methods that concrete implementations must implement\n",
    "    # --------------------------------------------------------------\n",
    "    @abstractmethod\n",
    "    def _load_config(self, config_file: Path) -> dict[str, Any]:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _file_type_to_load(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns file extension that identifies which files in config directory should be loaded.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class YamlConfigLoader(AbstractConfigLoader):\n",
    "    @property\n",
    "    def _file_type_to_load(self) -> str:\n",
    "        return 'yaml'\n",
    "\n",
    "    def _load_config(self, config_file: Path) -> dict[str, Any]:\n",
    "        with open(config_file, 'r') as file:\n",
    "            return yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockConfigLoader(AbstractConfigLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        shared_config_dict: dict[str, Any],\n",
    "        step_configs_dicts: list[dict[str, Any]],\n",
    "    ):\n",
    "        self._shared_config_dict = shared_config_dict\n",
    "        self._step_configs_dicts = step_configs_dicts\n",
    "\n",
    "    # Disable type checking, because we are overwriting a `final` method with a mock implementation.\n",
    "    @cached_property # type: ignore[misc]\n",
    "    def shared_config_as_dict(self) -> dict[str, Any]:\n",
    "        return self._shared_config_dict\n",
    "\n",
    "    # Disable type checking, because we are overwriting a `final` method with a mock implementation.\n",
    "    @cached_property # type: ignore[misc]\n",
    "    def step_configs_as_dicts(self) -> list[dict[str, Any]]:\n",
    "        return self._step_configs_dicts\n",
    "\n",
    "    # The following two methods are not needed, but are required to make the class *concrete*. While we could override this with a type: `ignore[abstract]`, it is better to avoid silencing type errors if easily possible.\n",
    "    @property\n",
    "    def _file_type_to_load(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _load_config(self, config_file: Path) -> dict[str, Any]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Pipeline* Facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "from sm_pipelines_oo.aws_connector.interface import AWSConnectorInterface\n",
    "from sm_pipelines_oo.aws_connector.implementation import create_aws_connector\n",
    "from sm_pipelines_oo.shared_config_schema import SharedConfig, Environment\n",
    "\n",
    "\n",
    "class PipelineFacade:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Environment,\n",
    "        config_loader: AbstractConfigLoader | None = None,\n",
    "    ):\n",
    "        self._env = env\n",
    "        # Allows providing a different config loader, especially for testing\n",
    "        self._user_provided_config_loader = config_loader\n",
    "\n",
    "        # Derived attributes\n",
    "        # ------------------\n",
    "        self._shared_config = SharedConfig(\n",
    "            **self._config_loader.shared_config_as_dict\n",
    "        )\n",
    "        self.aws_connector: AWSConnectorInterface = create_aws_connector(\n",
    "            shared_config=self._shared_config,\n",
    "            environment=env,\n",
    "        )\n",
    "        self.step_factory_facade = StepFactoryFacade(\n",
    "            step_config_dicts=self._config_loader.step_configs_as_dicts, # todo: pass in method call again?\n",
    "            role_arn=self.aws_connector.role_arn,\n",
    "            pipeline_session=self.aws_connector.pipeline_session,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _config_loader(self) -> AbstractConfigLoader:\n",
    "        if self._user_provided_config_loader is not None:\n",
    "            return self._user_provided_config_loader\n",
    "        else:\n",
    "            return YamlConfigLoader(env=self._env)\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"This is the main way user will interact with this class.\"\"\"\n",
    "        self._pipeline.upsert(\n",
    "            role_arn=self.aws_connector.role_arn,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _pipeline(self):\n",
    "        steps: list[ConfigurableRetryStep] = self.step_factory_facade.create_all_steps()\n",
    "        return Pipeline(\n",
    "            name=self._shared_config.project_name,\n",
    "            # parameters=[],\n",
    "            steps=steps,\n",
    "            sagemaker_session=self.aws_connector.pipeline_session,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of library code\n",
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_shared_config_dict={\n",
    "    'project_name': 'test',\n",
    "    'project_version': '0',\n",
    "    'region': 'us-east-1',\n",
    "    'project_bucket_name': '',\n",
    "}\n",
    "_fw_processor_config_dict = {\n",
    "    'step_name': 'preprocessing',\n",
    "    'step_factory_class': 'FrameworkProcessor',\n",
    "    'processor_init_args': {\n",
    "        'framework_version': '0.23-1',\n",
    "        'estimator_cls_name': 'SKLearn',\n",
    "        'instance_count': 1,\n",
    "        'instance_type': 'ml.m5.xlarge',\n",
    "    },\n",
    "    'processor_run_args': {\n",
    "        'code': 'preprocess.py',\n",
    "        'source_dir': 'code/preprocess',\n",
    "        'input_files_s3paths': [],\n",
    "        'output_files_s3paths': [],\n",
    "    },\n",
    "    'shared_config': {\n",
    "        'project_name': 'test',\n",
    "        'project_version': '0',\n",
    "        'region': 'local',\n",
    "        'project_bucket_name': 'test-bucket',\n",
    "    }\n",
    "}\n",
    "_mock_config_loader = MockConfigLoader(\n",
    "    shared_config_dict=_shared_config_dict,\n",
    "    step_configs_dicts=[_fw_processor_config_dict,]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.tokens:Loading cached SSO token for ml\n",
      "\u001b[32m2024-02-22 12:05:21.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msm_pipelines_oo.aws_connector.implementation\u001b[0m:\u001b[36mrole_arn\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mrole: arn:aws:iam::338755209567:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AdministratorAccess_7b40736629c71dd9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thomas-22/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.tokens:Loading cached SSO token for ml\n",
      "\u001b[32m2024-02-22 12:05:23.443\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msm_pipelines_oo.aws_connector.implementation\u001b[0m:\u001b[36mrole_arn\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mrole: arn:aws:iam::338755209567:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AdministratorAccess_7b40736629c71dd9\u001b[0m\n",
      "INFO:botocore.tokens:Loading cached SSO token for ml\n",
      "INFO:sagemaker.processing:Uploaded code/preprocess to s3://sagemaker-us-east-1-338755209567/test/code/c4e73b419a6046db5b2c11efa195e8e5/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-338755209567/test/code/bc2536a25d34e1ecae5238f42f4207c2/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:botocore.tokens:Loading cached SSO token for ml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded code/preprocess to s3://sagemaker-us-east-1-338755209567/test/code/c4e73b419a6046db5b2c11efa195e8e5/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-338755209567/test/code/bc2536a25d34e1ecae5238f42f4207c2/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    }
   ],
   "source": [
    "# import pdb; pdb.set_trace()\n",
    "p = PipelineFacade(\n",
    "    env='dev',\n",
    "    # Use different configs for testing\n",
    "    config_loader=_mock_config_loader,\n",
    ")\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm-pipelines-oo-tWfBw0_D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
